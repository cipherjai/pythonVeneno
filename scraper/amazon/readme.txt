Scrapy, as an application framework follows a project structure along with Object Oriented style of programming to define items and spiders for overall applications. The project structure which scrapy creates for a user has,
scrapy.cfg : It is a project configuration file which contains information for setting module for the project along with its deployment information.
test_project : It is an application directory with many different files which are actually responsible for running and scraping data from web urls.
items.py : Items are containers that will be loaded with the scraped data; they work like simple Python dicts. While one can use plain Python dicts with Scrapy, Items provide additional protection against populating undeclared fields, preventing typos. They are declared by creating a scrapy.Item class and defining its attributes as scrapy.Field objects.
pipelines.py : After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.Each item pipeline component is a Python class which has to implement a method called process_item to process scraped items. It receives an item and performs an action on it, also decides if the item should continue through the pipeline or should be dropped and and not processed any longer. If it wants to drop an item then it raises DropItem exception to drop it.
settings.py : It allows one to customise the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves. It provides a global namespace of key-value mappings that the code can use to pull configuration values from.
spiders :  Spiders is a directory which contains all spiders/crawlers as Python classes. Whenever one runs/crawls any spider then scrapy looks into this directory and tries to find the spider with its name provided by user. Spiders define how a certain site or a group of sites will be scraped, including how to perform the crawl and how to extract data from their pages. In other words, Spiders are the place where one defines the custom behavior for crawling and parsing pages for a particular site.Spiders have to define three major attributes i.e start_urls which tells which URLs are to be scrapped, allowed_domains which defines  only those domain names which need to scraped and parse is a method which is called when any response comes from lodged requests. These attributes are important because these constitute the base of Spider definitions.
